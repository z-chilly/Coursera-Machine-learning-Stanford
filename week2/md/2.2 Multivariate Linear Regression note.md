1、   多变量线性回归(Linear Regression with Multiple Variables)

### 4.1 多维特征

参考视频: 4 - 1 - Multiple Features (8 min).mkv

目前为止，我们探讨了单变量/特征的回归模型，现在我们对房价模型增加更多的特征，例如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0enbx1oj301r00i0np.jpg)。

![图片包含 瓶子  描述已自动生成](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0elj9u9j30gf06vmxd.jpg)

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0el2iqrj300700i0bf.jpg) 代表特征的数量

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0esjf9gj300f00j0f6.jpg)代表第 ![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0eiq1lsj300400i09q.jpg) 个训练实例，是特征矩阵中的第![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0eiq1lsj300400i09q.jpg)行，是一个**向量**（**vector**）。

比方说，上图的

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0eocft5j301r01hgld.jpg)，

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0egwvzdj300f00n0ha.jpg)代表特征矩阵中第 ![img](file:////Users/zhouhanqi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image009.png) 行的第 ![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ega5ugj300400i09o.jpg) 个特征，也就是第 ![img](file:////Users/zhouhanqi/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image009.png) 个训练实例的第 ![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ega5ugj300400i09o.jpg) 个特征。

如上图的![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0etejxxj302500m3y9.jpg)，

支持多变量的假设 ![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0enva2vj300600i0al.jpg) 表示为：![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0eudn6lj304l00iq2p.jpg)，

这个公式中有![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ek4cssj300p00i0ey.jpg)个参数和![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0emv2i7j300600i0ah.jpg)个变量，为了使得公式能够简化一些，引入![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0eoshtmj300u00i0g2.jpg)，则公式转化为：![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0er6dwuj304v00iq2p.jpg)

此时模型中的参数是一个![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ek4cssj300p00i0ey.jpg)维的向量，任何一个训练实例也都是![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ek4cssj300p00i0ey.jpg)维的向量，特征矩阵![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ehwzi2j300700i0as.jpg)的维度是 ![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0eprljuj301g00i0o5.jpg)。 因此公式可以简化为：![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ej7uc7j301m00j0s8.jpg)，其中上标![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0eq7pdkj300600i0a4.jpg)代表矩阵转置。

![/var/folders/7j/l70rjhts1_74y184b5l9lmnr0000gn/T/com.microsoft.Word/Content.MSO/D76A740C.tmp](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0f4jvfmj30qo0f0760.jpg)

 

### 4.2 多变量梯度下降

参考视频: 4 - 2 - Gradient Descent for Multiple Variables (5 min).mkv

与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ij113tj305600pa9u.jpg) ，

其中：![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ifnwxrj305h00j3ya.jpg) ，

我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。 多变量线性回归的批量梯度下降算法为：

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ihjdhzj308j03ca9v.jpg)

即：

![图片包含 物体  描述已自动生成](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0iihd4tj30b103i744.jpg)

求导数后得到：

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ignk5vj30ad058gli.jpg)

当![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ijtdywj300y00i0gn.jpg)时，

 ![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ikd2r3j304w00odfm.jpg)

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0iddtd8j304r015mwy.jpg)

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ilajpfj304r015mwy.jpg)

我们开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛。

代码示例：

计算代价函数 ![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ii0h33j304100p3ya.jpg) 其中：![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0iku344j305r00j742.jpg)

**Python** 代码：

def computeCost(X, y, theta):
   inner = np.power(((X * theta.T) - y), 2)
   return np.sum(inner) / (2 * len(X))

**![/var/folders/7j/l70rjhts1_74y184b5l9lmnr0000gn/T/com.microsoft.Word/Content.MSO/EDB451FA.tmp](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ienx5lj30qo0f040o.jpg)** 

### 4.3 梯度下降法实践1-特征缩放

参考视频: 4 - 3 - Gradient Descent in Practice I - Feature Scaling (9 min).mkv

在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。

以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0-2000平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。

![图片包含 文字  描述已自动生成](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0j8qb7tj305k06uwem.jpg)

解决的方法是尝试将所有特征的尺度都尽量缩放到-1到1之间。如图：

![图片包含 文字  描述已自动生成](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jcgv3wj30go09igo7.jpg)

最简单的方法是令：![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jalppsj301c00o0oy.jpg)，其中 ![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jbet05j300b00i0cl.jpg)是平均值，![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0j9mp1lj300a00i0c1.jpg)是标准差。

### 4.4 梯度下降法实践2-学习率

参考视频: 4 - 4 - Gradient Descent in Practice II - Learning Rate (9 min).mkv

梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。

![图片包含 文字  描述已自动生成](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jk0f6cj309y09m0t2.jpg)

也有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值（例如0.001）进行比较，但通常看上面这样的图表更好。

**梯度下降算法的每次迭代受到学习率的影响，如果学习率**![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ji0o01j300700i0al.jpg)**过小，则达到收敛所需的迭代次数会非常高；如果学习率**![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ji0o01j300700i0al.jpg)**过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。**

通常可以考虑尝试些学习率：

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jie5q6j304t00ldfl.jpg)

### 4.5 特征和多项式回归

参考视频: 4 - 5 - Features and Polynomial Regression (8 min).mkv

如房价预测问题，

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0js6gvzj305k04oq3k.jpg)

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0ju8db8j305b00i0si.jpg) 

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jwldlsj301w00i0sh.jpg)（临街宽度），![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jyt043j301f00i0o4.jpg)（纵向深度），![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jzd9kzj303t00ijr5.jpg)（面积），则：![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jssi7uj302600i3y9.jpg)。 

线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，比如一个二次方模型：![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jy01k7j303800jgld.jpg) 

或者三次方模型： ![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jtp92ej304600jt8h.jpg) 

![图片包含 文字, 地图  描述已自动生成](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jvt0o4j30ea07r3yr.jpg)

通常我们需要先观察数据然后再决定准备尝试怎样的模型。 另外，我们可以令：

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jxf546j302200j0s1.jpg)，从而将模型转化为线性回归模型。

根据函数图形特性，我们还可以使：

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jtbc4dj304b00jt8h.jpg)

或者:

![img](https://tva1.sinaimg.cn/large/006tNbRwgy1g9y0jurl1dj304400kt8h.jpg)

注：如果我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。